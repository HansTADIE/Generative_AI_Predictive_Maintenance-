"""
Module d'entra√Ænement du mod√®le pr√©dictif pour la maintenance
Utilise plusieurs algorithmes et compare leurs performances
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve
)
import joblib
import os
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class MaintenancePredictor:
    """Classe pour entra√Æner et √©valuer les mod√®les de pr√©diction de pannes"""
    
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.results = {}
        
    def load_training_data(self, data_dir='../data/train_test'):
        """
        Charge les donn√©es d'entra√Ænement et de test
        
        Args:
            data_dir: r√©pertoire contenant les donn√©es
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        try:
            X_train = pd.read_csv(os.path.join(data_dir, 'X_train.csv'))
            X_test = pd.read_csv(os.path.join(data_dir, 'X_test.csv'))
            y_train = pd.read_csv(os.path.join(data_dir, 'y_train_failure.csv')).values.ravel()
            y_test = pd.read_csv(os.path.join(data_dir, 'y_test_failure.csv')).values.ravel()
            
            print(f"‚úÖ Donn√©es charg√©es :")
            print(f"   - X_train: {X_train.shape}")
            print(f"   - X_test: {X_test.shape}")
            print(f"   - Distribution y_train: {np.bincount(y_train)}")
            print(f"   - Distribution y_test: {np.bincount(y_test)}")
            
            return X_train, X_test, y_train, y_test
        
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement : {e}")
            return None, None, None, None
    
    def initialize_models(self):
        """
        Initialise plusieurs mod√®les pour comparer leurs performances
        """
        self.models = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'  # Important pour les classes d√©s√©quilibr√©es
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42
            ),
            'Logistic Regression': LogisticRegression(
                max_iter=1000,
                random_state=42,
                class_weight='balanced'
            )
        }
        
        print(f"\n‚úÖ {len(self.models)} mod√®les initialis√©s :")
        for name in self.models.keys():
            print(f"   - {name}")
    
    def train_model(self, model_name, X_train, y_train):
        """
        Entra√Æne un mod√®le sp√©cifique
        
        Args:
            model_name: nom du mod√®le √† entra√Æner
            X_train: features d'entra√Ænement
            y_train: target d'entra√Ænement
        """
        print(f"\nüîÑ Entra√Ænement de {model_name}...")
        
        model = self.models[model_name]
        model.fit(X_train, y_train)
        
        print(f"‚úÖ {model_name} entra√Æn√©")
        
        return model
    
    def evaluate_model(self, model_name, model, X_test, y_test):
        """
        √âvalue les performances d'un mod√®le
        
        Args:
            model_name: nom du mod√®le
            model: mod√®le entra√Æn√©
            X_test: features de test
            y_test: target de test
            
        Returns:
            dict avec les m√©triques de performance
        """
        # Pr√©dictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # Calcul des m√©triques
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0
        }
        
        # Matrice de confusion
        cm = confusion_matrix(y_test, y_pred)
        
        # Stocker les r√©sultats
        self.results[model_name] = {
            'metrics': metrics,
            'confusion_matrix': cm,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'classification_report': classification_report(y_test, y_pred)
        }
        
        # Afficher les r√©sultats
        print(f"\nüìä R√©sultats pour {model_name} :")
        print(f"   - Accuracy:  {metrics['accuracy']:.4f}")
        print(f"   - Precision: {metrics['precision']:.4f}")
        print(f"   - Recall:    {metrics['recall']:.4f}")
        print(f"   - F1-Score:  {metrics['f1_score']:.4f}")
        print(f"   - ROC-AUC:   {metrics['roc_auc']:.4f}")
        
        return metrics
    
    def train_and_evaluate_all(self, X_train, X_test, y_train, y_test):
        """
        Entra√Æne et √©value tous les mod√®les
        
        Args:
            X_train, X_test, y_train, y_test: donn√©es d'entra√Ænement et de test
        """
        print("\n" + "="*60)
        print("üöÄ ENTRA√éNEMENT ET √âVALUATION DE TOUS LES MOD√àLES")
        print("="*60)
        
        best_f1 = 0
        
        for model_name in self.models.keys():
            # Entra√Æner le mod√®le
            model = self.train_model(model_name, X_train, y_train)
            
            # √âvaluer le mod√®le
            metrics = self.evaluate_model(model_name, model, X_test, y_test)
            
            # Garder le meilleur mod√®le (bas√© sur F1-score)
            if metrics['f1_score'] > best_f1:
                best_f1 = metrics['f1_score']
                self.best_model = model
                self.best_model_name = model_name
        
        print("\n" + "="*60)
        print(f"üèÜ MEILLEUR MOD√àLE : {self.best_model_name}")
        print(f"   F1-Score : {best_f1:.4f}")
        print("="*60)
    
    def plot_comparison(self, save_path='../models/comparison.png'):
        """
        Cr√©e un graphique comparant les performances des mod√®les
        
        Args:
            save_path: chemin o√π sauvegarder le graphique
        """
        metrics_names = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
        model_names = list(self.results.keys())
        
        # Pr√©parer les donn√©es pour le graphique
        data = []
        for model_name in model_names:
            for metric in metrics_names:
                data.append({
                    'Model': model_name,
                    'Metric': metric.replace('_', ' ').title(),
                    'Score': self.results[model_name]['metrics'][metric]
                })
        
        df_plot = pd.DataFrame(data)
        
        # Cr√©er le graphique
        plt.figure(figsize=(12, 6))
        sns.barplot(data=df_plot, x='Metric', y='Score', hue='Model')
        plt.title('Comparaison des Performances des Mod√®les', fontsize=16, fontweight='bold')
        plt.ylabel('Score', fontsize=12)
        plt.xlabel('M√©trique', fontsize=12)
        plt.ylim(0, 1)
        plt.legend(title='Mod√®le', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        
        # Sauvegarder
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\n‚úÖ Graphique de comparaison sauvegard√© : {save_path}")
        plt.close()
    
    def plot_confusion_matrix(self, model_name=None, save_path='../models/confusion_matrix.png'):
        """
        Affiche la matrice de confusion pour un mod√®le
        
        Args:
            model_name: nom du mod√®le (si None, utilise le meilleur)
            save_path: chemin o√π sauvegarder le graphique
        """
        if model_name is None:
            model_name = self.best_model_name
        
        cm = self.results[model_name]['confusion_matrix']
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['Pas de panne', 'Panne'],
                    yticklabels=['Pas de panne', 'Panne'])
        plt.title(f'Matrice de Confusion - {model_name}', fontsize=14, fontweight='bold')
        plt.ylabel('Vraie Classe', fontsize=12)
        plt.xlabel('Classe Pr√©dite', fontsize=12)
        plt.tight_layout()
        
        # Sauvegarder
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Matrice de confusion sauvegard√©e : {save_path}")
        plt.close()
    
    def get_feature_importance(self, X_train):
        """
        Affiche l'importance des features pour le meilleur mod√®le
        
        Args:
            X_train: features d'entra√Ænement (pour les noms de colonnes)
            
        Returns:
            DataFrame avec l'importance des features
        """
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            feature_names = X_train.columns
            
            df_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values('Importance', ascending=False)
            
            print(f"\nüìä Importance des Features ({self.best_model_name}) :")
            print(df_importance.to_string(index=False))
            
            # Graphique
            plt.figure(figsize=(10, 6))
            sns.barplot(data=df_importance, x='Importance', y='Feature', palette='viridis')
            plt.title(f'Importance des Features - {self.best_model_name}', 
                     fontsize=14, fontweight='bold')
            plt.xlabel('Importance', fontsize=12)
            plt.ylabel('Feature', fontsize=12)
            plt.tight_layout()
            
            save_path = '../models/feature_importance.png'
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Graphique d'importance sauvegard√© : {save_path}")
            plt.close()
            
            return df_importance
        else:
            print(f"‚ö†Ô∏è  {self.best_model_name} ne supporte pas feature_importances_")
            return None
    
    def save_best_model(self, save_path='../models/best_model.pkl'):
        """
        Sauvegarde le meilleur mod√®le
        
        Args:
            save_path: chemin o√π sauvegarder le mod√®le
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        model_info = {
            'model': self.best_model,
            'model_name': self.best_model_name,
            'metrics': self.results[self.best_model_name]['metrics'],
            'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        joblib.dump(model_info, save_path)
        print(f"\n‚úÖ Meilleur mod√®le sauvegard√© : {save_path}")
        print(f"   Mod√®le : {self.best_model_name}")
        print(f"   F1-Score : {model_info['metrics']['f1_score']:.4f}")


def main():
    """Fonction principale pour entra√Æner les mod√®les"""
    
    print("üöÄ ENTRA√éNEMENT DU MOD√àLE PR√âDICTIF DE MAINTENANCE")
    print("="*60)
    
    # Initialiser le predictor
    predictor = MaintenancePredictor()
    
    # Charger les donn√©es
    X_train, X_test, y_train, y_test = predictor.load_training_data()
    
    if X_train is None:
        print("‚ùå Impossible de continuer sans donn√©es")
        print("üí° Ex√©cutez d'abord preprocessing.py")
        return
    
    # Initialiser les mod√®les
    predictor.initialize_models()
    
    # Entra√Æner et √©valuer tous les mod√®les
    predictor.train_and_evaluate_all(X_train, X_test, y_train, y_test)
    
    # Cr√©er les visualisations
    predictor.plot_comparison()
    predictor.plot_confusion_matrix()
    predictor.get_feature_importance(X_train)
    
    # Sauvegarder le meilleur mod√®le
    predictor.save_best_model()
    
    print("\n" + "="*60)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS !")
    print("="*60)
    print("\nüìÅ Fichiers g√©n√©r√©s :")
    print("   - models/best_model.pkl")
    print("   - models/comparison.png")
    print("   - models/confusion_matrix.png")
    print("   - models/feature_importance.png")


if __name__ == "__main__":
    main()